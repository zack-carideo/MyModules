{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zjc10\\Desktop\\Projects\\envs\\embed_db\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
      "INFO:faiss.loader:Loading faiss.\n",
      "INFO:faiss.loader:Successfully loaded faiss.\n",
      "INFO:__main__:|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File DOES exist:\n",
      "\t c:\\Users\\zjc10\\Desktop\\Projects\\code\\MyModules\\semantic_search\\genq_pinecone\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sys,os,logging, gc\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, T5Tokenizer,T5TokenizerFast, T5ForConditionalGeneration\n",
    "import torch \n",
    "#set up basic logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logger =  logging.getLogger(__name__)\n",
    "\n",
    "#config path \n",
    "root_ = os.path.abspath(\"\")\n",
    "cfg_path = Path(root_) / \"config.yaml\"\n",
    "\n",
    "#custom imports\n",
    "sys.path.append(root_)\n",
    "from util.misc import LoadCFG, seed_all\n",
    "from util.data import load_data\n",
    "from util.embedding_ops import query_ops\n",
    "from util.model_ops import build_model \n",
    "from util.index_ops import ScalableSemanticSearch\n",
    "\n",
    "#set seed \n",
    "SEED = 42\n",
    "seed_all(SEED)\n",
    "\n",
    "#load cfg params\n",
    "cfg = LoadCFG(cfg_path, base_dir = root_).load()\n",
    "DATA_PATH = cfg.data.input.data_path\n",
    "SAVE_DIR = Path(cfg.data.output.data_save_dir)\n",
    "MODEL_SAVE_DIR = Path(cfg.model.model_save_dir)\n",
    "\n",
    "NSAMPS = cfg.model.n_samps\n",
    "TOK_BATCH_SIZE = cfg.model.tokenizer.batch_size\n",
    "BI_ENCODER_MODEL_NAME = cfg.model.bi_encoder.model_name\n",
    "EPOCHS = cfg.model.n_epochs\n",
    "BI_ENCODER_BATCH_SIZE =  cfg.model.bi_encoder.batch_size\n",
    "\n",
    "\n",
    "#device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#tokenizer setup\n",
    "return_tensors = cfg.model.tokenizer.return_tensors\n",
    "padding =  cfg.model.tokenizer.padding\n",
    "return_overflow_tokens= cfg.model.tokenizer.return_overflow_tokens\n",
    "max_seq_len = cfg.model.tokenizer.max_seq_len\n",
    "truncation = cfg.model.tokenizer.truncation \n",
    "stride = cfg.model.tokenizer.stride \n",
    "\n",
    "#query generator setup\n",
    "GENQ_MODEL_NAME = cfg.model.query_gen.model_name \n",
    "N_QUERIES_PER_PASSAGE =  cfg.model.query_gen.n_queries_per_passage \n",
    "\n",
    "#clean up gpu \n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "logger.info(torch.cuda.memory_summary(device='cuda', abbreviated=True))\n",
    "\n",
    "#create output folder if it doesnt exist\n",
    "if not SAVE_DIR.is_dir():\n",
    "    assert (not SAVE_DIR.is_file()), f'a directory to save outputs must be passed, you passed a full file path: {save_dir}'\n",
    "    if not SAVE_DIR.parent.is_dir(): \n",
    "        os.mkdir(str(SAVE_DIR.parent))\n",
    "        os.mkdir(str(SAVE_DIR))\n",
    "    else:\n",
    "        os.mkdir(str(SAVE_DIR))\n",
    "    logger.info(f\"new output directory created:{SAVE_DIR}\")\n",
    "\n",
    "if not MODEL_SAVE_DIR.is_dir():\n",
    "    assert SAVE_DIR.parent.is_dir(), f'parent directory: {SAVE_DIR} does not exist'\n",
    "    os.mkdir(str(MODEL_SAVE_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:loading data from huggyface\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': ['5733be284776f41900661182'],\n",
       " 'title': ['University_of_Notre_Dame'],\n",
       " 'context': ['Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'],\n",
       " 'question': ['To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?'],\n",
       " 'answers': [{'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.info('loading data from huggyface')\n",
    "df = load_data( load_from_directory=False \n",
    "               , hf_dataset_name = 'squad' \n",
    "               , split ='train') \n",
    "\n",
    "df[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:extracting text passages to generate queries for\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "The beginning of the Neolithic culture is considered to be in the Levant (Jericho, modern-day West Bank) about 10,200 – 8,800 BC. It developed directly from the Epipaleolithic Natufian culture in the region, whose people pioneered the use of wild cereals, which then evolved into true farming. The Natufian period was between 12,000 and 10,200 BC, and the so-called \"proto-Neolithic\" is now included in the Pre-Pottery Neolithic (PPNA) between 10,200 and 8,800 BC. As the Natufians had become dependent on wild cereals in their diet, and a sedentary way of life had begun among them, the climatic changes associated with the Younger Dryas are thought to have forced people to develop farming.\n"
     ]
    }
   ],
   "source": [
    "logging.info('extracting text passages to generate queries for')\n",
    "passages = list(set(df['context']))[:NSAMPS]\n",
    "print(len(passages))\n",
    "print(passages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:creating tokenizer and model to use in bi-encoder\n",
      "INFO:__main__:creating model to use in bi-encoder\n",
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "logger.info('creating tokenizer and model to use in bi-encoder')\n",
    "logger.info('creating model to use in bi-encoder')\n",
    "#tokenizer  = T5Tokenizer.from_pretrained(GENQ_MODEL_NAME, legacy=False) \n",
    "qgen_model = T5ForConditionalGeneration.from_pretrained(GENQ_MODEL_NAME)\n",
    "tokenizer = T5TokenizerFast.from_pretrained(GENQ_MODEL_NAME, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:forcing model into eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#call eval() to force / ensure model is running in 'INFERENCE MODE' and not 'TRAINING' mode\n",
    "logger.info('forcing model into eval mode')\n",
    "qgen_model.eval()\n",
    "model = qgen_model.to(device)\n",
    "#tokenizer = tokenizer\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:initalize embedding querier\n"
     ]
    }
   ],
   "source": [
    "#initalize class to generate queries from passages\n",
    "logger.info('initalize embedding querier')\n",
    "queryer = query_ops(\n",
    "     tokenizer\n",
    "    , qgen_model \n",
    "    , SAVE_DIR\n",
    "    , n_queries_per_passage = N_QUERIES_PER_PASSAGE\n",
    "    , save_batch_size = 1000\n",
    "    , train_batch_size = TOK_BATCH_SIZE    \n",
    "    , return_tensors = return_tensors\n",
    "    , padding =  padding\n",
    "    , return_overflowing_tokens= return_overflow_tokens\n",
    "    , max_seq_len = max_seq_len\n",
    "    , truncation = truncation \n",
    "    , stride = stride \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:generating query, passage key value pairs\n",
      "2it [00:18,  9.40s/it]\n"
     ]
    }
   ],
   "source": [
    "#generate query,passage key value pairs , save to disk , return paths \n",
    "logger.info('generating query, passage key value pairs')\n",
    "query_passage_outpaths = queryer.gen_queries_from_passages(passages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'In 2012, resident foreigners made up 23.3% of the population. Most of these (64%) were from European Union or EFTA countries. Italians were the largest single group of foreigners with 15.6% of total foreign population. They were closely followed by Germans (15.2%), immigrants from Portugal (12.7%), France (5.6%), Serbia (5.3%), Turkey (3.8%), Spain (3.7%), and Austria (2%). Immigrants from Sri Lanka, most of them former Tamil refugees, were the largest group among people of Asian origin (6.3%). Additionally, the figures from 2012 show that 34.7% of the permanent resident population aged 15 or over in Switzerland, i.e. 2,335,000 persons, had an immigrant background. A third of this population (853,000) held Swiss citizenship. Four fifths of persons with an immigration background were themselves immigrants (first generation foreigners and native-born and naturalised Swiss citizens), whereas one fifth were born in Switzerland (second generation foreigners and native-born and naturalised Swiss citizens). In the 2000s, domestic and international institutions expressed concern about what they perceived as an increase in xenophobia, particularly in some political campaigns. In reply to one critical report the Federal Council noted that \"racism unfortunately is present in Switzerland\", but stated that the high proportion of foreign citizens in the country, as well as the generally unproblematic integration of foreign</s>',\n",
       "  'doc': 2,\n",
       "  'ec_query_ids': tensor([[    0,   125,  1157,   485,    19,     3,     7, 15686,    15,  7721,\n",
       "               1,     0,     0,     0,     0,     0,     0],\n",
       "          [    0,  2015, 16096,    12,     3,     7, 15686,    15,  7721,     1,\n",
       "               0,     0,     0,     0,     0,     0,     0],\n",
       "          [    0,   125,  1093,    13,     3,     7, 15686,    15,  7721,    31,\n",
       "               7,  5169,    33,  2959,  2170,    58,     1]]),\n",
       "  'ec_query_txt': ['what nationality is switzerland',\n",
       "   'largest immigrants to switzerland',\n",
       "   \"what percent of switzerland's citizens are foreign born?\"]},\n",
       " {'text': 'in some political campaigns. In reply to one critical report the Federal Council noted that \"racism unfortunately is present in Switzerland\", but stated that the high proportion of foreign citizens in the country, as well as the generally unproblematic integration of foreigners\", underlined Switzerland\\'s openness.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       "  'doc': 2,\n",
       "  'ec_query_ids': tensor([[    0,   125,    19,     8,  1750,   344,     3,     7, 15686,    15,\n",
       "            7721,    11, 19343,     1,     0,     0,     0],\n",
       "          [    0,   125,    19,     8,  4903,    13, 21681,    16,     3,     7,\n",
       "           15686,    15,  7721,     1,     0,     0,     0],\n",
       "          [    0,   125,    19,     3,     7, 15686,    15,  7721,    31,     7,\n",
       "           10653,  1291,    58,     1,     0,     0,     0]]),\n",
       "  'ec_query_txt': ['what is the difference between switzerland and canada',\n",
       "   'what is the definition of racism in switzerland',\n",
       "   \"what is switzerland's immigration policy?\"]}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queryer._passage2chunk_map[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:creating training data for bi-encoder fine tuning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('where did the neolithic period begin',\n",
       "  'The beginning of the Neolithic culture is considered to be in the Levant (Jericho, modern-day West Bank) about 10,200 – 8,800 BC. It developed directly from the Epipaleolithic Natufian culture in the region, whose people pioneered the use of wild cereals, which then evolved into true farming. The Natufian period was between 12,000 and 10,200 BC, and the so-called \"proto-Neolithic\" is now included in the Pre-Pottery Neolithic (PPNA) between 10,200 and 8,800 BC. As the Natufians had become dependent on wild cereals in their diet, and a sedentary way of life had begun among them, the climatic changes associated with the Younger Dryas are thought to have forced people to develop farming.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'),\n",
       " ('where did the neolithic period begin',\n",
       "  'The beginning of the Neolithic culture is considered to be in the Levant (Jericho, modern-day West Bank) about 10,200 – 8,800 BC. It developed directly from the Epipaleolithic Natufian culture in the region, whose people pioneered the use of wild cereals, which then evolved into true farming. The Natufian period was between 12,000 and 10,200 BC, and the so-called \"proto-Neolithic\" is now included in the Pre-Pottery Neolithic (PPNA) between 10,200 and 8,800 BC. As the Natufians had become dependent on wild cereals in their diet, and a sedentary way of life had begun among them, the climatic changes associated with the Younger Dryas are thought to have forced people to develop farming.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'),\n",
       " ('where was the neolithic period located',\n",
       "  'The beginning of the Neolithic culture is considered to be in the Levant (Jericho, modern-day West Bank) about 10,200 – 8,800 BC. It developed directly from the Epipaleolithic Natufian culture in the region, whose people pioneered the use of wild cereals, which then evolved into true farming. The Natufian period was between 12,000 and 10,200 BC, and the so-called \"proto-Neolithic\" is now included in the Pre-Pottery Neolithic (PPNA) between 10,200 and 8,800 BC. As the Natufians had become dependent on wild cereals in their diet, and a sedentary way of life had begun among them, the climatic changes associated with the Younger Dryas are thought to have forced people to develop farming.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'),\n",
       " ('what is a multiprocessor pc',\n",
       "  'Some computers are designed to distribute their work across several CPUs in a multiprocessing configuration, a technique once employed only in large and powerful machines such as supercomputers, mainframe computers and servers. Multiprocessor and multi-core (multiple CPUs on a single integrated circuit) personal and laptop computers are now widely available, and are being increasingly used in lower-end markets as a result.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'),\n",
       " (\"what's a multiprocessing computer\",\n",
       "  'Some computers are designed to distribute their work across several CPUs in a multiprocessing configuration, a technique once employed only in large and powerful machines such as supercomputers, mainframe computers and servers. Multiprocessor and multi-core (multiple CPUs on a single integrated circuit) personal and laptop computers are now widely available, and are being increasingly used in lower-end markets as a result.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'),\n",
       " ('what is multi core',\n",
       "  'Some computers are designed to distribute their work across several CPUs in a multiprocessing configuration, a technique once employed only in large and powerful machines such as supercomputers, mainframe computers and servers. Multiprocessor and multi-core (multiple CPUs on a single integrated circuit) personal and laptop computers are now widely available, and are being increasingly used in lower-end markets as a result.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'),\n",
       " ('what is the language in egypt',\n",
       "  'The official language of the Republic is Modern Standard Arabic. Arabic was adopted by the Egyptians after the Arab invasion of Egypt. The spoken languages are: Egyptian Arabic (68%), Sa\\'idi Arabic (29%), Eastern Egyptian Bedawi Arabic (1.6%), Sudanese Arabic (0.6%), Domari (0.3%), Nobiin (0.3%), Beja (0.1%), Siwi and others. Additionally, Greek, Armenian and Italian are the main languages of immigrants. In Alexandria in the 19th century there was a large community of Italian Egyptians and Italian was the \"lingua franca\" of the city.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'),\n",
       " ('which language do most egyptians speak',\n",
       "  'The official language of the Republic is Modern Standard Arabic. Arabic was adopted by the Egyptians after the Arab invasion of Egypt. The spoken languages are: Egyptian Arabic (68%), Sa\\'idi Arabic (29%), Eastern Egyptian Bedawi Arabic (1.6%), Sudanese Arabic (0.6%), Domari (0.3%), Nobiin (0.3%), Beja (0.1%), Siwi and others. Additionally, Greek, Armenian and Italian are the main languages of immigrants. In Alexandria in the 19th century there was a large community of Italian Egyptians and Italian was the \"lingua franca\" of the city.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create sentence_transformers comptable training dataset using InputExample() method from transformers\n",
    "logger.info('creating training data for bi-encoder fine tuning')\n",
    "pairs = queryer.create_training_data( query_passage_outpaths)\n",
    "pairs[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:creating loader to handle loading batches of data for model training\n",
      "INFO:__main__:building model\n",
      "Some weights of MPNetModel were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['mpnet.pooler.dense.weight', 'mpnet.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n",
      "Iteration: 100%|██████████| 23/23 [00:25<00:00,  1.11s/it]\n",
      "Iteration: 100%|██████████| 23/23 [00:25<00:00,  1.09s/it]\n",
      "Iteration: 100%|██████████| 23/23 [00:26<00:00,  1.13s/it]\n",
      "Epoch: 100%|██████████| 3/3 [01:16<00:00, 25.55s/it]\n",
      "INFO:sentence_transformers.SentenceTransformer:Save model to C:\\Users\\zjc10\\Desktop\\Projects\\data\\semantic_search\\models\\fine_tuned_biencoder\n"
     ]
    }
   ],
   "source": [
    "#create object to handle loading of InputExample() instances in batches of 50 \n",
    "logger.info('creating loader to handle loading batches of data for model training')\n",
    "\n",
    "#build and train the bi-encoder to be used for asymetric search (information retrieval)\n",
    "#the trained model will encode passages into embeddings that are trained to be queried via short questions (as oppposed to just blindly taking the cossime between a short a long seq of text)\n",
    "logger.info('building model')\n",
    "ir_model = build_model(pairs\n",
    "                    , BI_ENCODER_MODEL_NAME\n",
    "                    , str(MODEL_SAVE_DIR / 'fine_tuned_biencoder')\n",
    "                    , epochs=EPOCHS\n",
    "                    , batch_size = BI_ENCODER_BATCH_SIZE\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2695563828.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[37], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    print( queryer._tokenizer.decode(outputs[0], skip_special_tokens=True)\u001b[0m\n\u001b[1;37m                                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "input_ids = queryer._tokenizer('where is egypt?',return_tensors='pt').input_ids\n",
    "outputs = ir_model.generate(input_ids)\n",
    "print( queryer._tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SentenceTransformer' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#build serachable index from all trained docs \u001b[39;00m\n\u001b[0;32m      2\u001b[0m input_ids \u001b[39m=\u001b[39m queryer\u001b[39m.\u001b[39m_tokenizer(\u001b[39m'\u001b[39m\u001b[39mwhere is egypt?\u001b[39m\u001b[39m'\u001b[39m,return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39minput_ids\n\u001b[1;32m----> 3\u001b[0m outputs \u001b[39m=\u001b[39m ir_model\u001b[39m.\u001b[39;49mgenerate(input_ids)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m( queryer\u001b[39m.\u001b[39m_tokenizer\u001b[39m.\u001b[39mdecode(outputs[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\zjc10\\Desktop\\Projects\\envs\\embed_db\\lib\\site-packages\\torch\\nn\\modules\\module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1205\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1206\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1207\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1208\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SentenceTransformer' object has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "#build serachable index from all trained docs \n",
    "input_ids = queryer._tokenizer('where is egypt?',return_tensors='pt').input_ids\n",
    "outputs = ir_model.generate(input_ids)\n",
    "print( queryer._tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed_db",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
