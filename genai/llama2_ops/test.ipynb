{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_cpp , sys , os, multiprocessing\n",
    "from llama_cpp import Llama\n",
    "sys.path.append('//home/zjc1002/Mounts/code/MyModules/utils')\n",
    "from hf_utils import download_model_hfhub\n",
    "from pathlib import Path \n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Information *(We will be using the below model and its prompt templates for this demo)*\n",
    "- **Model Name:** Mistral-7B-Instruct V0.2\n",
    "- **Model Card:** [hf model card](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/blob/main/)\n",
    "- **Context Window:** 32K \n",
    "- **Rope-thea:** 1e6\n",
    "- No Sliding Window Attention \n",
    "\n",
    "#### Model Prompt Templates \n",
    "- Description: In order to leverage instruction fine-tuning, your prompt should be surrounded by [INST] and [/INST] tokens. The very first instruction should begin with a begin of sentence id. The next instructions should not. The assistant generation will be ended by the end-of-sentence token id.\n",
    "\n",
    "- Standard Instruction Template\n",
    "\n",
    "    ```python\n",
    "    ext = \"<s>[INST] What is your favourite condiment? [/INST]\"\n",
    "    \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> \"\n",
    "    \"[INST] Do you have mayonnaise recipes? [/INST]\"\n",
    "    ```\n",
    "\n",
    "- Chat & Generation Templates: *this hf funcitonality can eliminate abiguity around what prompt template is needed for each model. the model tokenizer actually contains a method to format input text into model specific prompt tempalte*\n",
    "    - [Chat Templates HF](https://huggingface.co/docs/transformers/main/chat_templating)\n",
    "        - Note: Generation template information can be found in same link, not all models require generation prompts\n",
    "\n",
    "    - **MIstral-7B-Instruct V0.2 Jinja Template**    \n",
    "        ```jinja\n",
    "        {% if messages[0]['role'] == 'system' %}\n",
    "\n",
    "            {% set loop_messages = messages[1:] %}\n",
    "            {% set system_message = messages[0]['content'] %}\n",
    "\n",
    "        {% elif false == true and not '<<SYS>>' in messages[0]['content'] %}\n",
    "            {% set loop_messages = messages %}\n",
    "            {% set system_message = 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\\\n\\\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\\\\'t know the answer to a question, please don\\\\'t share false information.' %}\n",
    "\n",
    "        {% else %}\n",
    "            {% set loop_messages = messages %}\n",
    "            {% set system_message = false %}\n",
    "\n",
    "        {% endif %}\n",
    "\n",
    "        {% for message in loop_messages %}\n",
    "            \n",
    "            {% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n",
    "                {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n",
    "            {% endif %}\n",
    "            \n",
    "            {% if loop.index0 == 0 and system_message != false %}\n",
    "                {% set content = '<<SYS>>\\\\n' + system_message + '\\\\n<</SYS>>\\\\n\\\\n' + message['content'] %}\n",
    "            {% else %}\n",
    "                {% set content = message['content'] %}\n",
    "            {% endif %}\n",
    "\n",
    "            {% if message['role'] == 'user' %}\n",
    "                {{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}\n",
    "            {% elif message['role'] == 'system' %}\n",
    "                {{ '<<SYS>>\\\\n' + content.strip() + '\\\\n<</SYS>>\\\\n\\\\n' }}\n",
    "            {% elif message['role'] == 'assistant' %}\n",
    "                {{ ' '  + content.strip() + ' ' + eos_token }}\n",
    "            {% endif %}\n",
    "\n",
    "        {% endfor %}\"\n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_dir:/home/zjc1002/Mounts/llms/TheBloke_Mistral-7B-Instruct-v0.2-GGUF already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "non_gguf_modelname= \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "llamacpp_chat_format =\"mistral-instruct\"\n",
    "\n",
    "local_dir = \"/home/zjc1002/Mounts/llms/TheBloke_Mistral-7B-Instruct-v0.2-GGUF\"\n",
    "filename = \"mistral-7b-instruct-v0.2.Q4_0.gguf\"\n",
    "context_window = 1024 #MISTRAL 7B \n",
    "\n",
    "## Download model to local directory\n",
    "download_model_hfhub(repo_id = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
    "    , filename = filename\n",
    "    , repo_type = \"model\"\n",
    "    , local_dir = local_dir \n",
    "    , local_dir_use_symlinks = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama CPP Model Object Parameter Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /home/zjc1002/Mounts/llms/TheBloke_Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3917.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1024\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   128.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    98.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '2'}\n"
     ]
    }
   ],
   "source": [
    "model = Llama(\n",
    "\n",
    "    model_path =  Path(local_dir,filename).as_posix()  # The path to the Llama model file being used\n",
    "    , n_ctx = context_window  # The context size of the model\n",
    "    #, n_gpu_layers = -1    # number of layers to offload to GPU (if -1 all layers are offloaded to GPU)\n",
    "    # , split_mode= 0     # 0: split the model into layers, 1: split the model into blocks\n",
    "    # , n_batch = None    # prompt processing maximum batch size\n",
    "    # , n_threads= multiprocessing.cpu_count()-4   # number of threads to use for generation \n",
    "    # , n_threads_batch = None #number of threads to use for batch processing\n",
    "    # , seed = llama_cpp.LLAMA_DEFAULT_SEED\n",
    "    # , rope_scaling_type = llama_cpp.LLAMA_ROPE_SCALING_TYPE_UNSPECIFIED #Rows on Processor Elements (ROPE): Objective is to distribute the rows of the model layers across multiple GPUS or other processing elements. This can help scale ttraining by allowing it to take advantage of parallel processing capabilities.  \n",
    "    # , pooling_type = llama_cpp.LLAMA_POOLING_TYPE_UNSPECIFIED  #None, Mean , CLS\n",
    "    # , logits_all  = False #return logits for all tokens, not just the last token (MUST BE TRUE FOR COMPLETION to return logprobs) *IMPORTANT*\n",
    "    # , embedding = False #embedding mode only \n",
    "    # , last_n_tokens_size = 64 # Maximum number of of tokens to keep in the last_n_tokens deque\n",
    "    # , lora_base = None #Optional path to base model, useful if using a quantized base model and you want to applyu LoRa to an f16 model \n",
    "    # , lora_path = None #path to LoRa file to apply the model \n",
    "     , chat_format = llamacpp_chat_format #String specifying the chat format to use when calling create_chat_completion. \n",
    "    # , chat_handler = None #Optional chat handler to use when calling create_chat_completion \n",
    "    # , draft_model = None #OPtional draft model to use for speculative decoding \n",
    "    # , tokenizer = None #optional tokenizer to override the default tokenizer from llama.cpp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU threads available: 12\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default JNIJA chat template for mistralai/Mistral-7B-Instruct-v0.2\n",
      "{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif false == true and not '<<SYS>>' in messages[0]['content'] %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\\'t know the answer to a question, please don\\'t share false information.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'system' %}{{ '<<SYS>>\\n' + content.strip() + '\\n<</SYS>>\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\n",
      "Default Chat Prompt\n",
      "<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s>[INST] I'd like to show off how chat templating works! [/INST]\n",
      "Default Generation Prompt\n",
      "<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s>[INST] I'd like to show off how chat templating works! [/INST]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3050.23 ms\n",
      "llama_print_timings:      sample time =     267.62 ms /   709 runs   (    0.38 ms per token,  2649.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3050.17 ms /    49 tokens (   62.25 ms per token,    16.06 tokens per second)\n",
      "llama_print_timings:        eval time =  118205.79 ms /   708 runs   (  166.96 ms per token,     5.99 tokens per second)\n",
      "llama_print_timings:       total time =  123241.75 ms /   757 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-3434bb33-afa9-4832-a168-2755abcf6c88',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1713401967,\n",
       " 'model': '/home/zjc1002/Mounts/llms/TheBloke_Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_0.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': ' Sure thing! Chat templating is a way to create reusable templates for common messages or responses in a conversational interface, such as a chatbot or messaging application. This can save time and effort by allowing you to define a set of pre-written messages that can be easily inserted into conversations.\\n\\nHere\\'s an example of how you might use chat templating in Python using the ChatterBot library:\\n\\nFirst, let\\'s define some templates for common greetings and farewells:\\n```python\\nimport chatterbot\\n\\nclass GreetingChatBot(chatterbot.ChatBot):\\n    def __init__(self, *args, **kwargs):\\n        super().__init__(*args, **kwargs)\\n\\n        self.greetings = [\\n            \"Hello! How can I help you today?\",\\n            \"Hi there! What\\'s on your mind?\",\\n            \"Hey! What do you need from me?\"\\n        ]\\n\\n        self.farewells = [\\n            \"Goodbye! Have a great day.\",\\n            \"See you later! Take care.\",\\n            \"Bye for now!\"\\n        ]\\n\\n        self.train(self.greetings + self.farewells)\\n\\nbot = GreetingChatBot(\"GreetingBot\")\\n```\\nNext, let\\'s define some rules for handling greetings and farewells:\\n```python\\nclass GreetingRule(chatterbot.rules.Rule):\\n    def __init__(self, *args, **kwargs):\\n        super().__init__(*args, **kwargs)\\n\\n    def respond_to(self, sentence):\\n        response = self.bot.get_random_response_from_list(self.bot.greetings)\\n        return response\\n\\nclass FarewellRule(chatterbot.rules.Rule):\\n    def __init__(self, *args, **kwargs):\\n        super().__init__(*args, **kwargs)\\n\\n    def respond_to(self, sentence):\\n        response = self.bot.get_random_response_from_list(self.bot.farewells)\\n        return response\\n\\nbot.add_rule(GreetingRule(), \"greetings\")\\nbot.add_rule(FarewellRule(), \"farewells\")\\n```\\nNow, when the bot receives a message containing a greeting or farewell keyword, it will respond with a random message from the corresponding template list:\\n```python\\nif __name__ == \\'__main__\\':\\n    bot.connect()\\n\\n    while True:\\n        input_sentence = input(\"You: \")\\n        response = bot.get_response(input_sentence)\\n        print(\"Bot:\", response)\\n\\n        if response in bot.farewells:\\n            break\\n```\\nThis is just a simple example, but you can use chat templating to create more complex responses as well. For instance, you could define templates for handling specific questions or topics, and use conditional logic to determine which template to use based on the user\\'s input.\\n\\nI hope this helps give you an idea of how chat templating works! Let me know if you have any other questions.'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 49, 'completion_tokens': 708, 'total_tokens': 757}}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load mistral tokenizer from huggyface to enable use of chat templates \n",
    "tokenizer = AutoTokenizer.from_pretrained(non_gguf_modelname)\n",
    "print(f'default JNIJA chat template for {non_gguf_modelname}')\n",
    "print(tokenizer.default_chat_template)\n",
    "\n",
    "#SAMPLE MESSAGES\n",
    "chat = [\n",
    "\n",
    "  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "\n",
    "  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "\n",
    "  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "print('Default Chat Prompt')\n",
    "print(tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False))\n",
    "\n",
    "print('Default Generation Prompt')\n",
    "print(tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True))\n",
    "\n",
    "\n",
    "\n",
    "model.create_chat_completion(messages = chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Generate a short biography about Ronald Dahl?. Please do not exceed 3 sentences in your response. [/INST]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1837.83 ms\n",
      "llama_print_timings:      sample time =      59.11 ms /   153 runs   (    0.39 ms per token,  2588.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1837.71 ms /    30 tokens (   61.26 ms per token,    16.32 tokens per second)\n",
      "llama_print_timings:        eval time =   24162.72 ms /   152 runs   (  158.97 ms per token,     6.29 tokens per second)\n",
      "llama_print_timings:       total time =   26362.08 ms /   182 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-280c0a0d-57c2-4143-82b8-718eb2aff802',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1713401769,\n",
       " 'model': '/home/zjc1002/Mounts/llms/TheBloke_Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_0.gguf',\n",
       " 'choices': [{'text': '<s>[INST] Generate a short biography about Ronald Dahl?. Please do not exceed 3 sentences in your response. [/INST] Ronald Dahl (September 13, 1916 â€“ November 23, 1990) was a British novelist, short story writer, poet, and screenwriter whose works have been translated into over 60 languages. He is best known for his children\\'s books, which include \"James and the Giant Peach,\" \"Charlie and the Chocolate Factory,\" \"Matilda,\" and \"The BFG.\" These fantastical stories, filled with quirky characters and rollicking adventure, have delighted readers of all ages for generations. Dahl\\'s unique storytelling style and imaginative plots continue to inspire and captivate audiences around the world.',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 30, 'completion_tokens': 152, 'total_tokens': 182}}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_text_from_prompt(user_prompt,\n",
    "                             model,\n",
    "                             tokenizer,\n",
    "                             max_tokens = 1000,\n",
    "                             temperature = 0.3,\n",
    "                             top_p = 0.1,\n",
    "                             echo = True,\n",
    "                             stop = [\"Q\", \"\\n\"]):\n",
    "\n",
    "\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    tempalted_messages = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    print(tempalted_messages)\n",
    "\n",
    "\n",
    "   # Define the parameters\n",
    "    model_output = model(\n",
    "       # user_prompt,\n",
    "       tempalted_messages,\n",
    "       max_tokens=max_tokens,\n",
    "       temperature=temperature,\n",
    "       top_p=top_p,\n",
    "       echo=echo,\n",
    "       stop=stop,\n",
    "    )\n",
    "\n",
    "    return model_output\n",
    "\n",
    "\n",
    "my_prompt = \"Generate a short biography about Ronald Dahl?. Please do not exceed 3 sentences in your response.\"\n",
    "\n",
    "generate_text_from_prompt(my_prompt, model, tokenizer, stop=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1004.72 ms\n",
      "llama_print_timings:      sample time =      12.28 ms /    35 runs   (    0.35 ms per token,  2849.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    5568.45 ms /    35 runs   (  159.10 ms per token,     6.29 tokens per second)\n",
      "llama_print_timings:       total time =    5649.20 ms /    36 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1004.72 ms\n",
      "llama_print_timings:      sample time =       5.74 ms /    18 runs   (    0.32 ms per token,  3134.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     356.78 ms /     5 tokens (   71.36 ms per token,    14.01 tokens per second)\n",
      "llama_print_timings:        eval time =    2688.38 ms /    17 runs   (  158.14 ms per token,     6.32 tokens per second)\n",
      "llama_print_timings:       total time =    3090.93 ms /    22 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' a subjective matter and depends on your career goals, academic interests, and personal circumstances'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text Completion: quick teste of text completion / awnsering differeent genres of questions\n",
    "model(\"(4+4453)*32=\",stop=[\".\"], max_tokens=1000)[\"choices\"][0]['text']\n",
    "model(\"the best LLM is\",stop=[\".\"], max_tokens=1000)[\"choices\"][0]['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /home/zjc1002/Mounts/llms/TheBloke_Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3917.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 64\n",
      "llama_new_context_with_model: n_batch    = 40\n",
      "llama_new_context_with_model: n_ubatch   = 40\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =     8.00 MiB\n",
      "llama_new_context_with_model: KV self size  =    8.00 MiB, K (f16):    4.00 MiB, V (f16):    4.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     6.26 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '2'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m tokenized_text \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtokenize(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExplain the solor system\u001b[39m\u001b[38;5;124m'\u001b[39m, add_bos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, special\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mdetokenize(tokenized_text)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.98\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/llama2_env/lib/python3.11/site-packages/llama_cpp/llama.py:718\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    698\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    699\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    700\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    714\u001b[0m     idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[1;32m    715\u001b[0m )\n\u001b[1;32m    717\u001b[0m sample_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stopping_criteria \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mstopping_criteria\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scores\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    722\u001b[0m tokens_or_none \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m token\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "model = Llama(\n",
    "\n",
    "    model_path =  Path(local_dir,filename).as_posix() , n_ctx = 40  )\n",
    "tokenized_text = model.tokenize(b'Explain the solor system', add_bos=True, special=False)\n",
    "model.detokenize(tokenized_text)\n",
    "\n",
    "\n",
    "for token in model.generate(tokenized_text, top_k=50 ,  top_p=.98 , temp=.9, frequency_penalty=.5, ):\n",
    "    print(model.detokenize([token]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
