{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Claude3 API Operations\n",
    "\n",
    "- Tutorials\n",
    "    - Intro to Claude Prompting: https://docs.anthropic.com/claude/docs/intro-to-prompting\n",
    "    - Metaprompt helper: https://docs.anthropic.com/claude/docs/helper-metaprompt-experimental\n",
    "    \n",
    "- 3 Claude Variants(context window): version (ranked from smallest to largest)\n",
    "    - Haiku(200k): claude-3-haiku-20240307\n",
    "    - Sonnet(200k): claude-3-sonnet-20240229\n",
    "    - Opus(200k): claude-3-opus-20240229\n",
    "    - Claude2.1(200k): claude-2.1\n",
    "    - Claudee2(100k): claude-2.0\n",
    "    \n",
    "- limitations\n",
    "    - Cannot open Links\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File DOES exist:\n",
      "\t /home/zjc1002/Mounts/code/MyModules/genai/claude3_ops/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display\n",
    "from docx import Document\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "_root = Path.cwd()\n",
    "_utils_dir = _root.parent.parent.as_posix()  #'/home/zjc1002/Mounts/code/MyModules/utils\n",
    "os.sys.path.append(_utils_dir)\n",
    "from utils.misc import LoadCFG\n",
    "config_path = (_root / \"config.yaml\").as_posix()\n",
    "cfg = LoadCFG(config_path, base_dir=_root.as_posix(), return_namespace=False).load()\n",
    "\n",
    "\n",
    "#Anthropic API config params\n",
    "api_key = cfg['api_key'] #my api key\n",
    "model = cfg['model'] #name of anthropic model to use \n",
    "max_tokens = cfg['max_tokens'] #max tokens in anthropic model response \n",
    "temperature = cfg['temperature'] #how diverse do you want to the responses to be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docx_text(file_path):\n",
    "    doc = Document(file_path)\n",
    "    text = \"\"\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text += paragraph.text + \"\\n\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create client object using API key. \n",
    "*This client can be used for text generation , access vision capabliity, and streaming* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = anthropic.Anthropic(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Claude Client Request \n",
    "\n",
    "- Configurable Parameters \n",
    "    - Temperature: used to control the randomness of the generated response from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Prompt Examples \n",
    "\n",
    "\n",
    "- Useful techniques \n",
    "    - **Stop Sequences** - A stop sequence is a string that stops the model from generating tokens. Specifying stop sequences is another way to control the length and structure of the model's response. For example, you can tell the model to generate lists that have no more than 10 items by adding \"11\" as a stop sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ContentBlock(text='Here\\'s a Python function that identifies all active network connections on a local computer and returns a list of dictionaries containing the connection details:\\n\\n```python\\nimport psutil\\nimport socket\\n\\ndef get_active_connections():\\n    connections = []\\n    \\n    for conn in psutil.net_connections():\\n        if conn.status == psutil.CONN_ESTABLISHED or conn.status == psutil.CONN_SYN_SENT:\\n            try:\\n                process = psutil.Process(conn.pid)\\n                process_name = process.name()\\n            except psutil.NoSuchProcess:\\n                process_name = \"Unknown\"\\n            \\n            local_address = conn.laddr.ip\\n            local_port = conn.laddr.port\\n            remote_address = conn.raddr.ip if conn.raddr else None\\n            remote_port = conn.raddr.port if conn.raddr else None\\n            \\n            connection = {\\n                \\'local_address\\': local_address,\\n                \\'local_port\\': local_port,\\n                \\'remote_address\\': remote_address,\\n                \\'remote_port\\': remote_port,\\n                \\'state\\': conn.status,\\n                \\'pid\\': conn.pid,\\n                \\'process_name\\': process_name\\n            }\\n            connections.append(connection)\\n    \\n    return connections\\n```\\n\\nExplanation:\\n\\n1. The function `get_active_connections()` is defined to retrieve the active network connections.\\n\\n2. It initializes an empty list called `connections` to store the connection details.\\n\\n3. It iterates over all network connections using `psutil.net_connections()`.\\n\\n4. For each connection, it checks if the connection status is either `psutil.CONN_ESTABLISHED` (established connection) or `psutil.CONN_SYN_SENT` (connection in the process of being established).\\n\\n5. If the connection meets the criteria, it retrieves the process information using `psutil.Process(conn.pid)` and gets the process name using `process.name()`. If the process is not found, it sets the process name as \"Unknown\".\\n\\n6. It extracts the local address, local port, remote address, and remote port from the connection object. If the remote address is not available (e.g., for listening sockets), it sets the remote address and port to `None`.\\n\\n7. It creates a dictionary called `connection` with the following keys: \\'local_address\\', \\'local_port\\', \\'remote_address\\', \\'remote_port\\', \\'state\\', \\'pid\\', and \\'process_name\\'. It populates the dictionary with the corresponding values.\\n\\n8. The `connection` dictionary is appended to the `connections` list.\\n\\n9. Finally, the function returns the `connections` list containing all the active network connections.\\n\\nTo use this function, you can simply call `get_active_connections()`, and it will return a list of dictionaries representing the active network connections on the local computer.\\n\\nNote: This function requires the `psutil` module, which provides cross-platform utilities for retrieving information about running processes and system utilization. You can install it using `pip install psutil`.', type='text')]\n"
     ]
    }
   ],
   "source": [
    "#create a function to identify all active network connections \n",
    "message = client.messages.create(\n",
    "    model=model\n",
    "    ,max_tokens=max_tokens\n",
    "    ,temperature=temperature\n",
    "    ,system=\"you are a AI coding expert with a keen eye for simplicity. Your task is to create a python function to identify all active network connections on a local computer. The function should return a list of dictionaries, each containing the following keys: 'local_address', 'local_port', 'remote_address', 'remote_port', 'state', 'pid', 'process_name'.\"\n",
    "    ,messages=[\n",
    "        {\"role\": \"user\", \"content\": \"write a python function to identify all active network connections on a local computer?\"}\n",
    "    ]\n",
    ")\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CoverLetter Revisor\n",
    "*define a prompt to effectivly refine a coverletter to better represent the domain and skillsets required for the role. The agent works to both simply the language and improve the domain relevance of the input coverletter*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our input prompt template for the task. (zjc 3/15)\n",
    "def build_input_prompt(coverletter):\n",
    "    \n",
    "    user_content = f\"\"\"You will be provided a coverletter for a data science managerial position at a financial instutution. \n",
    "    Your job consists of 2 components: 1) Improve the quality of the coverletter based on your knowlodge of how data science and generative AI is applied within financial instututions. 2) Simplify the coverletter by removing duplicate or unnecessary information and correcting grammatical errors.  \n",
    "          \n",
    "    Specifically your goal is to improve the semantic flow of the coverletter to better represent the requirments of a data science manager working within the financial instrustry. You should also ensure the coverletter is free of any grammatical errors. You can use the following as a guide to help you evaluate the coverletter: \n",
    "\n",
    "    1) Does the coverletter effectively communicate the candidate's experience and skills in data science?\n",
    "    2) Does the coverletter effectively communicate the candidate's understanding of the financial industry?\n",
    "    3) Does the coverletter effectively communicate the candidate's understanding of the role of a data science manager?\n",
    "    4) Does the coverletter effectively describe the candidate as a leader that can sucessfuly manage a team of data scientists?\n",
    "\n",
    "    Here is the coverletter.\n",
    "    <coverletter>{coverletter}</coverletter>\n",
    "    \n",
    "    Please produce a revised version of the coverletter with your expert revisions. Do not include any comments or feedback in your response. Your response cannot exceed 4000 tokens.\"\"\"\n",
    "\n",
    "    \n",
    "    messages = [{'role': 'user', 'content': user_content}]\n",
    "    return messages\n",
    "\n",
    "\n",
    "# Define our get_completion function (including the stop sequence discussed above(Your response cannot exceed 4000 tokens.)).\n",
    "def get_completion(messages, model=None, max_tokens = 5):\n",
    "    response = client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=max_tokens,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get coverletter text\n",
    "file_path = \"/home/zjc1002/Mounts/data/coverletter/Zachary Carideo CoverLetter 2024.docx\"\n",
    "coverletter_text = load_docx_text(file_path)\n",
    "\n",
    "# Get completions for each input.\n",
    "output = get_completion(build_input_prompt(), model=model)\n",
    "#print it out \n",
    "print(output.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve the grammmer and flow of any sentence/paragraph or chunk of text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "_text = \"Throughout my experience as a manager and data scientist, I have learned the key to generating value lies in a manager's ability to foster a high-performing team environment and  strategize with leaders across business units to identify new opportunities to drive value and mitigate operational risks through the implementation of AI/ML initiatives. \"\n",
    "\n",
    "# Define our input prompt template for the task. (zjc 3/15)\n",
    "def build_input_prompt(text):\n",
    "    \n",
    "    user_content = f\"\"\"You will be provided a peice of text. Your job consists of 2 components: 1) Improve the grammer of the text , 2) re-phrase the text in a simplified manner that consolidates the original text.   \n",
    "    <text>{text}</text>\n",
    "    Please produce a revised version of the text with your expert revisions. Do not include any comments or feedback in your response.\"\"\"\n",
    "\n",
    "   \n",
    "    messages = [{'role': 'user', 'content': user_content}]\n",
    "    return messages\n",
    "\n",
    "\n",
    "# Define our get_completion function (including the stop sequence discussed above(Your response cannot exceed 4000 tokens.)).\n",
    "def get_completion(messages, model=None, max_tokens = 5,temperature=temperature):\n",
    "    response = client.messages.create(\n",
    "      model=model,\n",
    "      max_tokens=max_tokens,\n",
    "      temperature=temperature,\n",
    "      system=\"Respond in short and clear sentences.\",      \n",
    "      messages=messages\n",
    "    )\n",
    "    return response#.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughout my career as a manager and data scientist, I have discovered that creating value depends on two critical factors. First, a manager must cultivate a high-performing team environment. Second, they must collaborate with leaders across various business units to pinpoint new opportunities. By doing so, they can drive value and reduce operational risks through the implementation of AI\n"
     ]
    }
   ],
   "source": [
    "# Get completions for each input and copy to clipboard.\n",
    "import pyperclip\n",
    "output = get_completion(build_input_prompt(_text), model=model, max_tokens=int(len(_text)/5), temperature=.6)\n",
    "#print it out \n",
    "print(output.content[0].text)\n",
    "pyperclip.copy(output.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarizing Web Page Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 96th Academy Awards ceremony, presented by the Academy of Motion Picture Arts and Sciences (AMPAS), took place on March 10, 2024 at the Dolby Theatre in Los Angeles. Comedian Jimmy Kimmel hosted the show for the fourth time.\n",
      "\n",
      "The nominations were announced on January 23, 2024, with Oppenheimer leading with 13 nominations, followed by Poor Things and Killers of the Flower Moon with 11 and 10, respectively. Oppenheimer won a leading seven awards, including Best Picture and Best Director.\n",
      "\n",
      "Notable nominees included Steven Spielberg, Martin Scorsese, Thelma Schoonmaker, composer John Williams, and Willie D. Burton. Several actors received their first Oscar nominations. The ceremony also featured the revival of a popular presenting format where five Oscar-winning actors took the stage together to introduce the current nominees in their respective categories.\n",
      "\n",
      "The telecast drew 19.5 million viewers in the United States, becoming the most watched awards show since 2020. The ceremony received mostly positive reviews from critics, with highlights including musical performances, the respective wins of Japanese films Godzilla Minus One and The Boy and the Heron, and the past acting winners presenting format.\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "def download(url):\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        page_content = response.text\n",
    "    else:\n",
    "        print(f\"Failed to fetch the web page. Status code: {response.status_code}\")\n",
    "        exit(1)\n",
    "    return page_content        \n",
    "    \n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/96th_Academy_Awards\"\n",
    "page_content = download(url)\n",
    "\n",
    "\n",
    "#prepare input for claude\n",
    "prompt = f\"<content>{page_content}</content>Please produce a concise summary of the web page content. The summary cannot be longer than 500 tokens\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "#generate the summary \n",
    "response = client.messages.create(\n",
    "    model=model ,\n",
    "    max_tokens=500,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "summary = response.content[0].text\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returning JSON From Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'28207': {'average_home_price': 1450000}, '28209': {'average_home_price': 1150000}, '28211': {'average_home_price': 950000}, '28204': {'average_home_price': 850000}, '28203': {'average_home_price': 800000}, '28202': {'average_home_price': 750000}, '28226': {'average_home_price': 700000}, '28270': {'average_home_price': 650000}, '28277': {'average_home_price': 600000}, '28105': {'average_home_price': 550000}}\n"
     ]
    }
   ],
   "source": [
    "from anthropic import Anthropic\n",
    "import json\n",
    "import re, json \n",
    "from pprint import pprint\n",
    "\n",
    "def extract_json(response):\n",
    "    json_start = response.index(\"{\")\n",
    "    json_end = response.rfind(\"}\")\n",
    "    return json.loads(response[json_start:json_end + 1])\n",
    "\n",
    "what_i_want = 'top 10 most expensive zipcodes in Charlotte, NC including there average home price'\n",
    "\n",
    "#incorporate json request into prompt to tell claude you want a json formatted response\n",
    "message = client.messages.create(\n",
    "    model=model,\n",
    "    max_tokens= 1000,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"Give me a JSON dict with {what_i_want}\"\n",
    "        },\n",
    "    ]\n",
    ").content[0].text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Prompt Helper \n",
    "*The metaprompt is particularly useful as a \"getting started\" tool or as a method to generate multiple prompt versions for a given task, making it easier to test a variety of initial prompt variations for your use case.*\n",
    "- https://github.com/anthropics/anthropic-cookbook/blob/main/misc/building_moderation_filter.ipynb\n",
    "- https://console.anthropic.com/settings/plans\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langy2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
